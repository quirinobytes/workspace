{
  "name": "crawler-js",
  "description": "Opensource Framework Crawler in Node.js",
  "homepage": "http://github.com/CrawlerJS/CrawlerJS",
  "version": "2.0.1",
  "keywords": [
    "crawler",
    "scrapy",
    "hacker",
    "crawlers",
    "robots",
    "robot",
    "dom",
    "extraction",
    "nsa",
    "bigdata"
  ],
  "bugs": {
    "url": "http://github.com/CrawlerJS/CrawlerJS/issues",
    "email": "rodrigorizando@gmail.com"
  },
  "main": "index.js",
  "author": {
    "name": "Rodrigo Matheus",
    "email": "rodrigorizando@gmail.com"
  },
  "repository": {
    "type": "git",
    "url": "git://github.com/CrawlerJS/CrawlerJS.git"
  },
  "dependencies": {
    "csvtojson": "0.3.11",
    "request": "2.34.0",
    "cheerio": "0.13.1",
    "mongoq": "0.2.3",
    "querystring": "0.2.0"
  },
  "license": "ISC",
  "readme": "###Crawler-js\n---\nOpen source crawler framework\n\nI was upset not to have something simple to extract information to do experiments. Thus was born the CrawlerJS, a platform that enables extract information from any websites without having to keep worrying about developing.\n\nRodrigo Matheus\n\n## Example to use\n\n```js\nvar crawlerjs = require('crawler-js');\n\nvar worlds = {\n  interval: 1000,\n  getSample: 'http://www.tibia.com/community/?subtopic=worlds',\n  get: 'http://www.tibia.com/community/?subtopic=worlds',\n  preview: 0,\n  extractors: [\n    {\n      selector: '.TableContentContainer table.TableContent tr',\n      callback: function(err, html, url, response){\n        console.log('Crawled url:');\n        console.log(url);\n        // console.log(response); // If you need see more details about request\n        if(!err){\n          data = {};\n          data.world = html.children('td').eq(0).children('a').attr('href');\n          if(typeof data.world == 'undefined'){\n            delete data.world;\n          }\n          console.log(data);\n        }else{\n          console.log(err);\n        }\n      }\n    }\n  ]\n}\n\ncrawlerjs(worlds);\n```\n\n## See more examples on Examples path\n",
  "readmeFilename": "README.md",
  "_id": "crawler-js@2.0.1",
  "_from": "crawler-js@"
}
